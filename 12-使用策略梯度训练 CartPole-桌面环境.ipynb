{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 楼 + 机器学习实战\n",
    "\n",
    "# 挑战：使用策略梯度训练 CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 以下内容仅保留挑战代码部分，挑战全文请到原课程查看。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**挑战**：参考「基于策略的强化学习方法」实验内容，尝试通过 Monte-Carlo 策略梯度算法训练 `CartPole-v1` 游戏。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "def mc_policy_gradient(env, theta, lr, episodes):\n",
    "    for episode in range(episodes):  # 迭代 episode\n",
    "        episode = []\n",
    "        start_observation = env.reset()  # 初始化环境\n",
    "        t = 0\n",
    "        while True:\n",
    "            env.render()  # notebook 不支持渲染环境\n",
    "            policy = np.dot(theta, start_observation)  # 计算策略值\n",
    "            # 这里的 action_space 为 2, 故使用 Sigmoid 激活函数处理策略值\n",
    "            pi = 1 / (1 + np.exp(-policy))\n",
    "            if pi >= 0.5:\n",
    "                action = 1  # 向右施加力\n",
    "            else:\n",
    "                action = 0  # 向左施加力\n",
    "            next_observation, reward, done, _ = env.step(action)  # 执行动作\n",
    "            # 将环境返回结果添加到 episode 中\n",
    "            episode.append([next_observation, action, pi, reward])\n",
    "            start_observation = next_observation  # 将返回 observation 作为下一次迭代 observation\n",
    "            t += 1\n",
    "            if done:\n",
    "                print(\"Episode finished after {} timesteps\".format(t))\n",
    "                break\n",
    "        # 根据上一次 episode 更新参数 theta\n",
    "        for timestep in episode:\n",
    "            observation, action, pi, reward = timestep\n",
    "            theta += lr * (1 - pi) * np.transpose(-observation) * reward\n",
    "    \n",
    "    return theta\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    lr = 0.005\n",
    "    theta = np.random.rand(4)\n",
    "    episodes=10\n",
    "    env = gym.make('CartPole-v1')\n",
    "    mc_policy_gradient(env, theta, lr, episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #999;font-size: 12px;font-style: italic;\">*本课程内容，由作者授权实验楼发布，未经允许，禁止转载、下载及非法传播。</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
